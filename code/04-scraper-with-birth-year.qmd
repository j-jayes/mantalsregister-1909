---
title: "Scraper"
author: "JJayes"
date: "16/11/2021"
format: html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(rvest)

# knitr::purl("code/02-ingest-1800.Rmd", documentation = 2)
```

## Purpose

Start with ingesting the data from the website.

### Robots.txt

```{r}
# library(robotstxt)
# get_robotstxt("https://stadsarkivet.stockholm")
```

There's nothing here, so we can proceed.

### Trying for mantalsregister 1800

Want to get the table with the first elements

```{r}
get_table_data <- function(url) {
  html <- read_html(url)
  
  message(paste0("Getting data from", url))
  
  table <- html %>%
    html_node(".table") %>%
    html_table()

  table <- table %>%
    janitor::clean_names() %>%
    filter(between(row_number(), 4, 13))

  table <- table %>%
    rename(
      district = x1,
      mantalsskrivnings_nr = x2,
      surname = x3,
      first_name = x4,
      birth_date = x5,
      title = x6,
      other = x7
    ) %>%
    select(1:7)
  
  # sleep a little between each request
  Sys.sleep(rnorm(1, 2, .2))
  
  table
  
}

# test
# test <- get_table_data("https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1855/Sok?sidindex=1")

```

Get n pages

```{r}
get_n_pages <- function(url) {
  html <- read_html(url)

  n_hits <- html %>%
    html_nodes(".antaltraffar-div") %>%
    html_text() %>%
    str_squish() %>%
    str_extract("[0-9].*") %>%
    str_remove_all(" ") %>%
    parse_number()

  n_pages <- floor(n_hits / 10)
  
  message("The number of pages is: ", n_pages)

  n_pages
}

```

List of all the links


```{r}
# year_search <- 1830
# base_url <- paste0("https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1800-1884/Sok?sidindex=21&artal=", year_search)
start_page <- 0

n_pages <- get_n_pages("https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1855/Sok?sidindex=1")

list_of_pages <- tibble(url = paste0("https://sok.stadsarkivet.stockholm.se/Databas/mantalsregister-1855/Sok?sidindex=",
                                     start_page:n_pages),
                        # make a page number just in case we want to check back later
                        # store it just as a number as this is small for file size sake.
                        page = start_page:n_pages)
```

purrr::map takes our url and applies the function to it methodically.

```{r}
library(dplyr)
library(purrr)
library(tidyr)

chunk_size <- 1000
num_chunks <- ceiling(nrow(list_of_pages) / chunk_size)

for (i in 1:num_chunks) {
  start_idx <- (i - 1) * chunk_size + 1
  end_idx <- min(i * chunk_size, nrow(list_of_pages))
  
  df_chunk <- list_of_pages[start_idx:end_idx, ] %>%
    mutate(data = map(url, possibly(get_table_data, "failed"))) %>%
    select(!url) %>%
    filter(!data == "failed")
  
  st <- format(Sys.time(), "%Y-%m-%d-%I-%M-%p")
  
  # Save chunked data
  df_chunk %>% write_rds(paste0("data/birth_year/df_", st, "_chunk_", i, ".rds"), compress = "gz")
  
  # Optional: Pause between requests to avoid overloading the server
  Sys.sleep(1)
}

```
